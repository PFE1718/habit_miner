{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import sklearn.model_selection as sk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(filename):\n",
    "    with open(filename, mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        mydict = {rows[0]:rows[1] for rows in reader}\n",
    "    return mydict\n",
    "\n",
    "my_meta_dict = read_metadata('metadata.csv')\n",
    "num_labels = int(my_meta_dict['nb_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "(64, 3)\n",
      "xtrain\n",
      "(64, 3)\n",
      "xtest\n",
      "(20, 3)\n",
      "xval\n",
      "(16, 3)\n",
      "ytrain\n",
      "(64, 17)\n",
      "yvalid\n",
      "(16, 17)\n",
      "ytest\n",
      "(20, 17)\n"
     ]
    }
   ],
   "source": [
    "def read_csv(my_csv):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(my_csv, newline='') as csvfile:\n",
    "         reader = csv.DictReader(csvfile)\n",
    "         for row in reader:\n",
    "             data.append([row['x'],row['y'],row['intent']])\n",
    "             labels.append(row['cluster'])\n",
    "             #print(row['x'], row['y'],row['cluster'],row['intent'])\n",
    "         X = np.array(data)\n",
    "         print(X.shape)\n",
    "         y = np.array(labels)\n",
    "         X_train, X_test, y_train, y_test = sk.train_test_split(X, y, test_size=0.2)\n",
    "         X_train, X_val, y_train, y_val = sk.train_test_split(\n",
    "             X_train, y_train, test_size=0.2, random_state=1)\n",
    "         print(X_train.shape)\n",
    "\n",
    "         return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "def string_to_index(my_list):\n",
    "    indexed = []\n",
    "    unique_vals = np.unique(my_list).tolist()\n",
    "    for i in range(len(my_list)):\n",
    "        indexed.append(float(unique_vals.index(my_list[i])))\n",
    "    indexed = np.array(indexed)\n",
    "    return indexed\n",
    "\n",
    "def change_data(my_array):\n",
    "    new_intent = string_to_index(my_array[:,2])\n",
    "    new_x = my_array[:,0].astype(np.float)\n",
    "    new_y = my_array[:,1].astype(np.float)\n",
    "\n",
    "    new_array = np.array((new_x,new_y,new_intent))\n",
    "    return new_array.transpose()\n",
    "\n",
    "def string_to_float(my_array):\n",
    "    return my_array.astype(np.float)\n",
    "\n",
    "##transform labels into arrays for training\n",
    "def to_one_hot(y, n_classes=num_labels):\n",
    "    y = y.astype(np.int)\n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = read_csv('clusters.csv')\n",
    "X_train = change_data(X_train)\n",
    "X_val = change_data(X_val)\n",
    "X_test = change_data(X_test)\n",
    "y_train = string_to_float(y_train)\n",
    "y_train = to_one_hot(y_train)\n",
    "y_val = string_to_float(y_val)\n",
    "y_val = to_one_hot(y_val)\n",
    "y_test = string_to_float(y_test)\n",
    "y_test = to_one_hot(y_test)\n",
    "print('xtrain')\n",
    "print(X_train.shape)\n",
    "print('xtest')\n",
    "print(X_test.shape)\n",
    "print('xval')\n",
    "print(X_val.shape)\n",
    "print('ytrain')\n",
    "print(y_train.shape)\n",
    "print('yvalid')\n",
    "print(y_val.shape)\n",
    "print('ytest')\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "nb_points = 64\n",
    "num_channels = 3\n",
    "now = datetime.now()\n",
    "logdir = \"./Classifier/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #x = tf.placeholder(tf.float32, [nb_points, 3], name='X')  # inputs\n",
    "    #y = tf.placeholder(tf.float32, [None, 1], name='Y')  # outputs\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=[batch_size,num_channels], name='train_dataset')\n",
    "    tf_train_labels = tf.placeholder(tf.float64, shape=[16,num_labels],name = 'train_labels')\n",
    "    tf_valid_dataset = tf.constant(X_val,dtype=tf.float32)\n",
    "    tf_test_dataset = tf.constant(X_test,dtype=tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    with tf.name_scope('hidden') as scope:\n",
    "        W1 = tf.Variable(tf.truncated_normal(\n",
    "            [num_channels, depth], stddev=0.1),dtype=tf.float32)\n",
    "        b1 = tf.Variable(tf.zeros([depth]),dtype=tf.float32)\n",
    "        W2 = tf.Variable(tf.truncated_normal(\n",
    "              [depth, depth], stddev=0.1),dtype=tf.float32)\n",
    "        b2 = tf.Variable(tf.constant(1.0, shape=[depth]),dtype=tf.float32)\n",
    "        W3 = tf.Variable(tf.truncated_normal(\n",
    "              [depth, num_hidden], stddev=0.1),dtype=tf.float32)\n",
    "        b3 = tf.Variable(tf.constant(1.0, shape=[num_hidden]),dtype=tf.float32)\n",
    "        W4 = tf.Variable(tf.truncated_normal(\n",
    "              [num_hidden, num_labels], stddev=0.1),dtype=tf.float32)\n",
    "        b4 = tf.Variable(tf.constant(1.0, shape=[num_labels]),dtype=tf.float32)\n",
    "    \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        with tf.name_scope('layers'):\n",
    "            # compute layer 1 \n",
    "            with tf.name_scope('A1'):\n",
    "                A1 = tf.nn.sigmoid(tf.matmul(data, W1) + b1)\n",
    "            #compute layer 2\n",
    "            with tf.name_scope('A2'):\n",
    "                A2 = tf.nn.sigmoid(tf.matmul(A1, W2) + b2)\n",
    "            #compute layer 3\n",
    "            with tf.name_scope('A3'):\n",
    "                A3 = tf.nn.sigmoid(tf.matmul(A2, W3) + b3)\n",
    "            #Prevent overfitting with drouput regularization\n",
    "            dropout= tf.nn.dropout(A3, keep_prob=0.5)\n",
    "            return (tf.matmul(dropout, W4) + b4)\n",
    "\n",
    "    #summaries for tensorboard\n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            #tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "    \n",
    "    #calculate accuracy\n",
    "    def accuracy(predictions, labels):\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                / predictions.shape[0])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    variable_summaries(loss)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Include some Tensorboard visualization\n",
    "    writer_train = tf.summary.FileWriter(logdir)\n",
    "    loss_summary = tf.summary.scalar('loss_summary',loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    with tf.name_scope('predictions'):\n",
    "        with tf.name_scope('train'):\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "        with tf.name_scope('valid'):\n",
    "            valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "        with tf.name_scope('test'):\n",
    "            test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    #merged = tf.summary.merge_all()\n",
    "    #variables_names =[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.615470\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "Minibatch loss at step 50: 1.403585\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 100: 2.252873\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 150: 0.927659\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 200: 1.440455\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 250: 2.507010\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 300: 0.925275\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 350: 1.492309\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 400: 2.217010\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 450: 0.906179\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 500: 1.428463\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 550: 2.196968\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 600: 0.966619\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 650: 1.428948\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 700: 2.138027\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 750: 0.968439\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 800: 1.445396\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 850: 2.121509\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 900: 0.947186\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 950: 1.609036\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 50.0%\n",
      "Test accuracy: 60.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  writer_train.add_graph(session.graph)\n",
    "  print('Initialized')\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    batch_data = X_train[offset:(offset + batch_size),:]\n",
    "    batch_labels = y_train[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions,ls = session.run(\n",
    "      [optimizer, loss, train_prediction,loss_summary], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), y_val))\n",
    "    \n",
    "    writer_train.add_summary(ls,step)\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
